[int]
src_max_len = 256
trg_max_len = 256
n_heads = 12
d_model = 768
d_ff = 3072
n_enc_layers = 12
n_dec_layers = 12
src_vocab_size = 45000
trg_vocab_size = 45000
beam_size = 5
max_decode_steps = 256

[float]
gamma = 0.65
#repetition_penalty = -4

[str]
src_vocab = model/vocab/zh_en_vocab.txt
trg_vocab = model/vocab/zh_en_vocab.txt
search_strategy = beam_search
src_tokenizer = mimix
trg_tokenizer = mimix
activation = gelu_new
model = transformer
task = enc_dec
load_model = model/nmt/nmt.base.model
normalize = linear

[bool]
use_cuda = False
share_src_trg_emb = True
share_emb_out_proj = True
norm_before_pred = True
use_proj_bias = False
use_pre_norm = True